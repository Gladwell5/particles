{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "from nn_helpers import ParticleDS, train_loop, test_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in using our custom dataset class (which is defined in the nn_helpers file)\n",
    "batch_size = 64\n",
    "\n",
    "#predictors = ['p', 'theta', 'beta', 'nphe', 'ein', 'eout']\n",
    "predictors = ['p_scaled', 'theta_scaled', 'beta_scaled', 'nphe_scaled', 'ein_scaled', 'eout_scaled']\n",
    "outcome = 'id'\n",
    "\n",
    "ds_size = '500k'\n",
    "train_ds = ParticleDS(f'../data/pid_{ds_size}_train_balanced.csv', predictors, outcome)\n",
    "test_ds = ParticleDS(f'../data/pid_{ds_size}_test.csv', predictors, outcome)\n",
    "\n",
    "print(Counter(train_ds.y))\n",
    "print(Counter(test_ds.y))\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural net is a sequence of layers with different numbers of parameters\n",
    "# This one has 4 layers\n",
    "# - an input linear layer with 6 inputs (the number of predictor variables) and n outputs (this is arbitrary)\n",
    "# - an Rectified Linear Unit layer which adjusts the results of the first layer to be 0 below 0\n",
    "# - a second linear layer with n inputs and 4 outputs (the number of particle types)\n",
    "# - a final layer that gets the most likely output of the 4 previous output values to give a final prediction\n",
    "class TinyModel(nn.Module):\n",
    "    def __init__(self, n_hidden=16, n_middle=0):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=6, out_features=n_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        for _ in range(n_middle):\n",
    "            self.model.append(nn.Linear(in_features=n_hidden, out_features=n_hidden))\n",
    "            self.model.append(nn.ReLU())\n",
    "        self.model.append(nn.Linear(in_features=n_hidden, out_features=4))\n",
    "        self.model.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the model here will start the training fresh each time\n",
    "# Skip it if you just want to continue on training the model as is\n",
    "nn_model = TinyModel(n_hidden=12, n_middle=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate is how fast the model learns and epochs are the number of times the full training data are passed through the model\n",
    "# Experimenting with these to get the best model is called hyperparameter tuning\n",
    "learning_rate = .1 # a good learning rate allows the training and test loss to come down in step with one another but not too slowly\n",
    "# set the learning rate higher when you have less data and lower when you have more to avoid overfitting\n",
    "epochs = 1000 # set epochs as high as you like for this task, it's an upper limit and we can continue as long as the test set accuracy is increasing (and/or test set loss is decreasing)\n",
    "\n",
    "# Cross Entropy Loss is a popular way to measure the difference between predicted categories and actual categories (aka loss)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Stochastic Gradient Descent is the method that updates the model parameters (aka how it learns)\n",
    "optimiser = torch.optim.SGD(nn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# The train loop and test loop are defined in our nn_helpers file - have a look at those to see what's happening in each loop\n",
    "accuracies = [0]\n",
    "for t in range(epochs):\n",
    "    print(('-' * 30) + f'\\nEpoch {t+1}')\n",
    "    train_loss = train_loop(train_dataloader, nn_model, loss_fn, optimiser, batch_size, print_freq=0)\n",
    "    accuracy, test_loss, avg_acc = test_loop(test_dataloader, nn_model, loss_fn)\n",
    "    print(f'loss: {train_loss:.3f} | {test_loss:.3f}, acc: {accuracy:.4f}, avg acc: {avg_acc:.4f}')\n",
    "    if avg_acc >= .75:\n",
    "        model_path = f'../models/nn_{str(avg_acc)[2:5]}_{ds_size.lower()}.pt'\n",
    "        if not os.path.exists(model_path):\n",
    "            torch.save(nn_model.state_dict(), model_path)\n",
    "    # break after avg_acc (weighted accuracy) on test set drops significantly\n",
    "    if len(accuracies) > 1:\n",
    "        if avg_acc/max(accuracies) <= .9:\n",
    "            print(\"Early stop - test accuracy drop\")\n",
    "            break\n",
    "    accuracies.append(avg_acc)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final performance metrics on the test set\n",
    "best_model_path = sorted(glob('../models/*_500k.pt'))[-1]\n",
    "print(best_model_path)\n",
    "weights_biases = torch.load(best_model_path)\n",
    "n_hidden = weights_biases['model.0.bias'].shape[0]\n",
    "n_middle = len(weights_biases)//2 - 2\n",
    "\n",
    "nn_model = TinyModel(n_hidden, n_middle)\n",
    "nn_model.load_state_dict(weights_biases)\n",
    "nn_model.eval()\n",
    "\n",
    "pred_y = torch.argmax(nn_model(test_ds.X), dim=1).detach().numpy()\n",
    "print(classification_report(test_ds.y, pred_y))\n",
    "accuracy = accuracy_score(test_ds.y, pred_y)\n",
    "\n",
    "conf_mat = confusion_matrix(test_ds.y, pred_y)\n",
    "print(conf_mat)\n",
    "class_acc = (conf_mat.diagonal()/conf_mat.sum(1))\n",
    "print(' | '.join([str(round(acc, 3)) for acc in class_acc]))\n",
    "print(f'avg. accuracy: {class_acc.mean():.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
