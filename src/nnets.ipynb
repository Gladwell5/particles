{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "from nn_helpers import ParticleDS, train_loop, test_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 1189, 1: 1189, 3: 1189, 0: 1189})\n",
      "Counter({1: 56238, 3: 38815, 0: 4650, 2: 297})\n"
     ]
    }
   ],
   "source": [
    "# Load the data in using our custom dataset class (which is defined in the nn_helpers file)\n",
    "batch_size = 64\n",
    "\n",
    "#predictors = ['p', 'theta', 'beta', 'nphe', 'ein', 'eout']\n",
    "predictors = ['p_scaled', 'theta_scaled', 'beta_scaled', 'nphe_scaled', 'ein_scaled', 'eout_scaled']\n",
    "outcome = 'id'\n",
    "\n",
    "ds_size = '500k'\n",
    "train_ds = ParticleDS(f'../data/pid_{ds_size}_train_balanced.csv', predictors, outcome)\n",
    "test_ds = ParticleDS(f'../data/pid_{ds_size}_test.csv', predictors, outcome)\n",
    "\n",
    "print(Counter(train_ds.y))\n",
    "print(Counter(test_ds.y))\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural net is a sequence of layers with different numbers of parameters\n",
    "# This one has 4 layers\n",
    "# - an input linear layer with 6 inputs (the number of predictor variables) and 16 outputs (this is arbitrary)\n",
    "# - an Rectified Linear Unit layer which adjusts the results of the first layer to be 0 below 0\n",
    "# - a second linear layer with 16 inputs and 4 outputs (the number of particle types)\n",
    "# - a final layer that gets the most likely output of the 4 previous output values to give a final prediction\n",
    "class TinyModel(nn.Module):\n",
    "    def __init__(self, n_hidden=16, n_middle=0):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=6, out_features=n_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        for _ in range(n_middle):\n",
    "            self.model.append(nn.Linear(in_features=n_hidden, out_features=n_hidden))\n",
    "            self.model.append(nn.ReLU())\n",
    "        self.model.append(nn.Linear(in_features=n_hidden, out_features=4))\n",
    "        self.model.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the model here will start the training fresh each time\n",
    "# Skip it if you just want to continue on training the model as is\n",
    "nn_model = TinyModel(n_hidden=12, n_middle=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Epoch 1\n",
      "loss: 0.750 | 0.823, acc: 0.9108, avg acc: 0.9333\n",
      "------------------------------\n",
      "Epoch 2\n",
      "loss: 0.750 | 0.823, acc: 0.9103, avg acc: 0.9335\n",
      "------------------------------\n",
      "Epoch 3\n",
      "loss: 0.750 | 0.823, acc: 0.9103, avg acc: 0.9336\n",
      "------------------------------\n",
      "Epoch 4\n",
      "loss: 0.750 | 0.823, acc: 0.9103, avg acc: 0.9336\n",
      "------------------------------\n",
      "Epoch 5\n",
      "loss: 0.750 | 0.823, acc: 0.9103, avg acc: 0.9336\n",
      "------------------------------\n",
      "Epoch 6\n",
      "loss: 0.750 | 0.823, acc: 0.9103, avg acc: 0.9336\n",
      "------------------------------\n",
      "Epoch 7\n",
      "loss: 0.750 | 0.823, acc: 0.9103, avg acc: 0.9336\n",
      "------------------------------\n",
      "Epoch 8\n",
      "loss: 0.750 | 0.823, acc: 0.9104, avg acc: 0.9337\n",
      "------------------------------\n",
      "Epoch 9\n",
      "loss: 0.750 | 0.823, acc: 0.9104, avg acc: 0.9336\n",
      "------------------------------\n",
      "Epoch 10\n",
      "loss: 0.750 | 0.823, acc: 0.9104, avg acc: 0.9337\n",
      "------------------------------\n",
      "Epoch 11\n",
      "loss: 0.750 | 0.823, acc: 0.9104, avg acc: 0.9337\n",
      "------------------------------\n",
      "Epoch 12\n",
      "loss: 0.750 | 0.823, acc: 0.9104, avg acc: 0.9337\n",
      "------------------------------\n",
      "Epoch 13\n",
      "loss: 0.750 | 0.823, acc: 0.9104, avg acc: 0.9337\n",
      "------------------------------\n",
      "Epoch 14\n",
      "loss: 0.750 | 0.823, acc: 0.9105, avg acc: 0.9337\n",
      "------------------------------\n",
      "Epoch 15\n",
      "loss: 0.750 | 0.823, acc: 0.9105, avg acc: 0.9338\n",
      "------------------------------\n",
      "Epoch 16\n",
      "loss: 0.750 | 0.823, acc: 0.9105, avg acc: 0.9337\n",
      "------------------------------\n",
      "Epoch 17\n",
      "loss: 0.750 | 0.823, acc: 0.9105, avg acc: 0.9338\n",
      "------------------------------\n",
      "Epoch 18\n",
      "loss: 0.750 | 0.823, acc: 0.9105, avg acc: 0.9338\n",
      "------------------------------\n",
      "Epoch 19\n",
      "loss: 0.750 | 0.823, acc: 0.9105, avg acc: 0.9338\n",
      "------------------------------\n",
      "Epoch 20\n",
      "loss: 0.750 | 0.823, acc: 0.9106, avg acc: 0.9338\n",
      "------------------------------\n",
      "Epoch 21\n",
      "loss: 0.750 | 0.823, acc: 0.9106, avg acc: 0.9338\n",
      "------------------------------\n",
      "Epoch 22\n",
      "loss: 0.750 | 0.823, acc: 0.9106, avg acc: 0.9339\n",
      "------------------------------\n",
      "Epoch 23\n",
      "loss: 0.750 | 0.823, acc: 0.9106, avg acc: 0.9338\n",
      "------------------------------\n",
      "Epoch 24\n",
      "loss: 0.750 | 0.823, acc: 0.9106, avg acc: 0.9339\n",
      "------------------------------\n",
      "Epoch 25\n",
      "loss: 0.750 | 0.823, acc: 0.9107, avg acc: 0.9339\n",
      "------------------------------\n",
      "Epoch 26\n",
      "loss: 0.750 | 0.823, acc: 0.9106, avg acc: 0.9339\n",
      "------------------------------\n",
      "Epoch 27\n",
      "loss: 0.750 | 0.823, acc: 0.9106, avg acc: 0.9339\n",
      "------------------------------\n",
      "Epoch 28\n",
      "loss: 0.750 | 0.823, acc: 0.9107, avg acc: 0.9339\n",
      "------------------------------\n",
      "Epoch 29\n",
      "loss: 0.750 | 0.823, acc: 0.9107, avg acc: 0.9340\n",
      "------------------------------\n",
      "Epoch 30\n",
      "loss: 0.750 | 0.823, acc: 0.9106, avg acc: 0.9339\n",
      "------------------------------\n",
      "Epoch 31\n",
      "loss: 0.750 | 0.823, acc: 0.9107, avg acc: 0.9340\n",
      "------------------------------\n",
      "Epoch 32\n",
      "loss: 0.750 | 0.823, acc: 0.9107, avg acc: 0.9340\n",
      "------------------------------\n",
      "Epoch 33\n",
      "loss: 0.750 | 0.823, acc: 0.9107, avg acc: 0.9340\n",
      "------------------------------\n",
      "Epoch 34\n",
      "loss: 0.750 | 0.823, acc: 0.9107, avg acc: 0.9340\n",
      "------------------------------\n",
      "Epoch 35\n",
      "loss: 0.750 | 0.823, acc: 0.9107, avg acc: 0.9340\n",
      "------------------------------\n",
      "Epoch 36\n",
      "loss: 0.750 | 0.823, acc: 0.9107, avg acc: 0.9340\n",
      "------------------------------\n",
      "Epoch 37\n",
      "loss: 0.750 | 0.823, acc: 0.9108, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 38\n",
      "loss: 0.750 | 0.823, acc: 0.9108, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 39\n",
      "loss: 0.750 | 0.823, acc: 0.9108, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 40\n",
      "loss: 0.750 | 0.823, acc: 0.9108, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 41\n",
      "loss: 0.750 | 0.823, acc: 0.9108, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 42\n",
      "loss: 0.750 | 0.823, acc: 0.9108, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 43\n",
      "loss: 0.750 | 0.823, acc: 0.9108, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 44\n",
      "loss: 0.750 | 0.823, acc: 0.9109, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 45\n",
      "loss: 0.750 | 0.823, acc: 0.9109, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 46\n",
      "loss: 0.750 | 0.823, acc: 0.9109, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 47\n",
      "loss: 0.750 | 0.823, acc: 0.9109, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 48\n",
      "loss: 0.750 | 0.823, acc: 0.9109, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 49\n",
      "loss: 0.750 | 0.823, acc: 0.9108, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 50\n",
      "loss: 0.750 | 0.823, acc: 0.9108, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 51\n",
      "loss: 0.750 | 0.823, acc: 0.9108, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 52\n",
      "loss: 0.750 | 0.823, acc: 0.9108, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 53\n",
      "loss: 0.750 | 0.823, acc: 0.9109, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 54\n",
      "loss: 0.750 | 0.823, acc: 0.9109, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 55\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 56\n",
      "loss: 0.750 | 0.823, acc: 0.9109, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 57\n",
      "loss: 0.750 | 0.823, acc: 0.9109, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 58\n",
      "loss: 0.750 | 0.823, acc: 0.9109, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 59\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 60\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 61\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 62\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 63\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 64\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 65\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 66\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 67\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 68\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 69\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 70\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 71\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 72\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 73\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 74\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 75\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 76\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 77\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 78\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 79\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 80\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 81\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 82\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 83\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 84\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 85\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 86\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 87\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 88\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 89\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 90\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 91\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 92\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 93\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 94\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 95\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 96\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 97\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 98\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 99\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 100\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 101\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 102\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 103\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 104\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 105\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 106\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 107\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 108\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 109\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 110\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 111\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 112\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 113\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 114\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 115\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 116\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 117\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 118\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 119\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 120\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 121\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 122\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 123\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 124\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 125\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 126\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 127\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 128\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 129\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 130\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 131\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 132\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 133\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 134\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 135\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 136\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 137\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 138\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 139\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 140\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 141\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 142\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 143\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 144\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 145\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 146\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 147\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 148\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 149\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 150\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 151\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 152\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 153\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 154\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 155\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 156\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 157\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 158\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 159\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 160\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 161\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 162\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 163\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 164\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 165\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 166\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 167\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 168\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 169\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 170\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 171\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 172\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 173\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 174\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 175\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 176\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 177\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 178\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 179\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 180\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 181\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 182\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 183\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 184\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 185\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 186\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 187\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 188\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 189\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 190\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 191\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 192\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 193\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 194\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 195\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 196\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 197\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 198\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 199\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 200\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 201\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 202\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 203\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 204\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 205\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 206\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 207\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 208\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 209\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 210\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 211\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 212\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 213\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 214\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 215\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 216\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 217\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 218\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 219\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 220\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 221\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 222\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 223\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 224\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 225\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 226\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 227\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 228\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 229\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 230\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 231\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 232\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 233\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 234\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9343\n",
      "------------------------------\n",
      "Epoch 235\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 236\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 237\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 238\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 239\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 240\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 241\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 242\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 243\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 244\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 245\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 246\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 247\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 248\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 249\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 250\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 251\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 252\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 253\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 254\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 255\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 256\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 257\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 258\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 259\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 260\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 261\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 262\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 263\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 264\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 265\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 266\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 267\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 268\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 269\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 270\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 271\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 272\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 273\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 274\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 275\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 276\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 277\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 278\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 279\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 280\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 281\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 282\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 283\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 284\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 285\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 286\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 287\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 288\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 289\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 290\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 291\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 292\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 293\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 294\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 295\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 296\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 297\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 298\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 299\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 300\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 301\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 302\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 303\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 304\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 305\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 306\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 307\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 308\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 309\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 310\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 311\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 312\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 313\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 314\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 315\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 316\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 317\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 318\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 319\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 320\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 321\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 322\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 323\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 324\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 325\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 326\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 327\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 328\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 329\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 330\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 331\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 332\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 333\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 334\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 335\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 336\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 337\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 338\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 339\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 340\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 341\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 342\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 343\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 344\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 345\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 346\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 347\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 348\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 349\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 350\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 351\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 352\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 353\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 354\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 355\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 356\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 357\n",
      "loss: 0.750 | 0.823, acc: 0.9112, avg acc: 0.9342\n",
      "------------------------------\n",
      "Epoch 358\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 359\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 360\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 361\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 362\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 363\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 364\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 365\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 366\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 367\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 368\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 369\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 370\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 371\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 372\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 373\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 374\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 375\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 376\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 377\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 378\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 379\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 380\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 381\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 382\n",
      "loss: 0.751 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 383\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 384\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 385\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 386\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 387\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 388\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 389\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 390\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 391\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 392\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 393\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 394\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9350\n",
      "------------------------------\n",
      "Epoch 395\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 396\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 397\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 398\n",
      "loss: 0.750 | 0.823, acc: 0.9110, avg acc: 0.9349\n",
      "------------------------------\n",
      "Epoch 399\n",
      "loss: 0.750 | 0.823, acc: 0.9111, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 400\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 401\n",
      "loss: 0.751 | 0.823, acc: 0.9110, avg acc: 0.9341\n",
      "------------------------------\n",
      "Epoch 402\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loop(train_dataloader, nn_model, loss_fn, optimiser, batch_size, print_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m accuracy, test_loss, avg_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, avg acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m avg_acc \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.75\u001b[39m:\n",
      "File \u001b[0;32m~/Repos/particles/src/nn_helpers.py:54\u001b[0m, in \u001b[0;36mtest_loop\u001b[0;34m(dataloader, model, loss_fn, loud)\u001b[0m\n\u001b[1;32m     51\u001b[0m test_loss, correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;66;03m# ensure no changes occur to the model during this loop\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     55\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X) \u001b[38;5;66;03m# get predictions\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# add to total loss over test set\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:195\u001b[0m, in \u001b[0;36mcollate_int_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_int_fn\u001b[39m(batch, \u001b[38;5;241m*\u001b[39m, collate_fn_map: Optional[Dict[Union[Type, Tuple[Type, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]], Callable]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Learning rate is how fast the model learns and epochs are the number of times the full training data are passed through the model\n",
    "# Experimenting with these to get the best model is called hyperparameter tuning\n",
    "learning_rate = .1 # a good learning rate allows the training and test loss to come down in step with one another but not too slowly\n",
    "# set the learning rate higher when you have less data and lower when you have more to avoid overfitting\n",
    "epochs = 1000 # set epochs as high as you like for this task, it's an upper limit and we can continue as long as the test set accuracy is increasing (and/or test set loss is decreasing)\n",
    "\n",
    "# Cross Entropy Loss is a popular way to measure the difference between predicted categories and actual categories (aka loss)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Stochastic Gradient Descent is the method that updates the model parameters (aka how it learns)\n",
    "optimiser = torch.optim.SGD(nn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# The train loop and test loop are defined in our nn_helpers file - have a look at those to see what's happening in each loop\n",
    "accuracies = [0]\n",
    "for t in range(epochs):\n",
    "    print(('-' * 30) + f'\\nEpoch {t+1}')\n",
    "    train_loss = train_loop(train_dataloader, nn_model, loss_fn, optimiser, batch_size, print_freq=0)\n",
    "    accuracy, test_loss, avg_acc = test_loop(test_dataloader, nn_model, loss_fn)\n",
    "    print(f'loss: {train_loss:.3f} | {test_loss:.3f}, acc: {accuracy:.4f}, avg acc: {avg_acc:.4f}')\n",
    "    if avg_acc >= .75:\n",
    "        model_path = f'../models/nn_{str(avg_acc)[2:5]}_{ds_size.lower()}.pt'\n",
    "        if not os.path.exists(model_path):\n",
    "            torch.save(nn_model.state_dict(), model_path)\n",
    "    # break after avg_acc (weighted accuracy) on test set drops significantly\n",
    "    if len(accuracies) > 1:\n",
    "        if avg_acc/max(accuracies) <= .9:\n",
    "            print(\"Early stop - test accuracy drop\")\n",
    "            break\n",
    "    accuracies.append(avg_acc)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/nn_791_5m.pt\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.96      0.52      4650\n",
      "           1       0.99      0.86      0.92     56238\n",
      "           2       0.09      0.38      0.14       297\n",
      "           3       0.98      0.95      0.97     38815\n",
      "\n",
      "    accuracy                           0.90    100000\n",
      "   macro avg       0.61      0.79      0.64    100000\n",
      "weighted avg       0.96      0.90      0.92    100000\n",
      "\n",
      "[[ 4482    75    12    81]\n",
      " [ 6211 48341  1131   555]\n",
      " [    0   184   113     0]\n",
      " [ 1979     1    10 36825]]\n",
      "0.964 | 0.86 | 0.38 | 0.949\n",
      "avg. accuracy: 0.788\n"
     ]
    }
   ],
   "source": [
    "# Get the final performance metrics on the test set\n",
    "best_model_path = sorted(glob('../models/*_500k.pt'))[-1]\n",
    "print(best_model_path)\n",
    "weights_biases = torch.load(best_model_path)\n",
    "n_hidden = weights_biases['model.0.bias'].shape[0]\n",
    "n_middle = len(weights_biases)//2 - 2\n",
    "\n",
    "nn_model = TinyModel(n_hidden, n_middle)\n",
    "nn_model.load_state_dict(weights_biases)\n",
    "nn_model.eval()\n",
    "\n",
    "pred_y = torch.argmax(nn_model(test_ds.X), dim=1).detach().numpy()\n",
    "print(classification_report(test_ds.y, pred_y))\n",
    "accuracy = accuracy_score(test_ds.y, pred_y)\n",
    "\n",
    "conf_mat = confusion_matrix(test_ds.y, pred_y)\n",
    "print(conf_mat)\n",
    "class_acc = (conf_mat.diagonal()/conf_mat.sum(1))\n",
    "print(' | '.join([str(round(acc, 3)) for acc in class_acc]))\n",
    "print(f'avg. accuracy: {class_acc.mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
