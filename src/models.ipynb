{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_size = '500k'\n",
    "df_train = pd.read_csv(f'../data/pid_{ds_size}_train.csv', low_memory=False)\n",
    "df_test = pd.read_csv(f'../data/pid_{ds_size}_test.csv', low_memory=False)\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look a the first few rows\n",
    "print(df_train.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting models requires us to split datasets into what we want to use as predictors and what we want to predict\n",
    "#predictor_variables = ['p', 'theta', 'beta', 'nphe', 'ein', 'eout']\n",
    "predictor_variables = ['p_scaled', 'theta_scaled', 'beta_scaled', 'nphe_scaled', 'ein_scaled', 'eout_scaled']\n",
    "outcome_variable = 'id'\n",
    "\n",
    "# The variables we want to use in prediction are conventionally named X\n",
    "X_train = df_train[predictor_variables]#.sample(frac=.1)\n",
    "X_test = df_test[predictor_variables]\n",
    "\n",
    "# The outcome categories should be sequential integers (0, 1, 2, etc.)\n",
    "# So we'll order the unique set of particle ids/types alphabetically...\n",
    "outcome_values = sorted(list(set(df_train['id'])))\n",
    "\n",
    "# ...and then use that to create the outcome vectors (conventionally named y)\n",
    "y_train = [outcome_values.index(_id) for _id in df_train[outcome_variable]]\n",
    "y_test = [outcome_values.index(_id) for _id in df_test[outcome_variable]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train is still a dataframe so you can use .head() to look at the first 5 rows\n",
    "print(X_train.head())\n",
    "print(X_train.describe())\n",
    "\n",
    "# y_train is a vector so you can use [i:j] syntax to look at the first 5 values\n",
    "print(y_train[:5])\n",
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the same for test\n",
    "print(X_test.head())\n",
    "print(X_test.describe())\n",
    "print(y_test[:5])\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can fit the multinomial logistic regression model and print the results on the test set\n",
    "model = LogisticRegression(random_state=1234, max_iter=2000, verbose=0, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then try a decision tree\n",
    "model = DecisionTreeClassifier(random_state=1234, class_weight=None)\n",
    "model.fit(X_train, y_train)\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the same with a random forest classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=10,\n",
    "    random_state=1234,\n",
    "    verbose=1,\n",
    "    criterion='entropy',\n",
    "    warm_start=True,\n",
    "    max_depth=None,\n",
    "    n_jobs=4,\n",
    "    class_weight=None\n",
    "    )\n",
    "classifier.fit(X_train, y_train)\n",
    "print('Model fit. Running predictions...')\n",
    "print(classification_report(y_test, classifier.predict(X_test)))\n",
    "print(confusion_matrix(y_test, classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LinearSVC(verbose=True, random_state=1234, class_weight='balanced')\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classification_report(y_test, classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SGDClassifier(random_state=1234, class_weight='balanced')\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classification_report(y_test, classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do experimentation by varying\n",
    "#   a) the scaled/unscaled variables\n",
    "#   b) the balanced/unbalanced training set\n",
    "#   c) the unbalanced training set but setting the class_weight argument of the models to 'balanced'\n",
    "#   d) other model parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
