{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_size = '500k'\n",
    "df = pd.read_csv(f'../data/pid-{ds_size}.csv', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lookup/dictionary for the particle names\n",
    "particle_dict = {\n",
    "    -11: 'positron',\n",
    "    211: 'pion',\n",
    "    2212: 'proton',\n",
    "    321: 'kaon'\n",
    "}\n",
    "\n",
    "# Use .value_counts() on categorical variables to show the count of each value\n",
    "print(df['id'].value_counts())\n",
    "\n",
    "# Use a list comprehension to overwrite the 'id' variable with the particle names\n",
    "df['id'] = [particle_dict[particle_id] for particle_id in df['id'].values]\n",
    "\n",
    "# Now it will show the names\n",
    "print(df['id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in scaled versions of the predictor varaiables at this point\n",
    "# We'll see why this might be useful when it comes to modelling\n",
    "predictor_variables = ['p', 'theta', 'beta', 'nphe', 'ein', 'eout']\n",
    "scaled_variables = [f'{var_name}_scaled' for var_name in predictor_variables]\n",
    "df[scaled_variables] = StandardScaler().fit_transform(df[predictor_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets, stratified by particle type/id\n",
    "# Use test_size argument to decide proportion of data to use for testing\n",
    "# Use random_state so you get the same splits every time you run this command\n",
    "df_train, df_test = train_test_split(df, test_size=0.20, random_state=1234, stratify=df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows is as expected\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "# Check the distribution of values in the particle id variable\n",
    "print(df_train['id'].value_counts())\n",
    "print(df_test['id'].value_counts())\n",
    "\n",
    "# The training counts should be approximately 4 times the test counts (since we split the data 4:1)\n",
    "print(df_test['id'].value_counts() * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.choice is used to select a sample from a larger set\n",
    "# Run this a few times to see the results then change some of the parameters\n",
    "np.random.choice(['a', 'b', 'c', 'd', 'e'], size=3, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the np.random.choice function to create balanced training data, where each type of particle has the same number of rows\n",
    "# We'll do this by defining a function called undersample as we'll want to use this a few times\n",
    "\n",
    "def undersample(dataframe, min_rows_per_type=100, scaling_power=0):\n",
    "    # We'll do this in a loop, where each iteration samples for a different particle\n",
    "    # First we make an empty list to hold the indexes we select\n",
    "    selected_indexes = []\n",
    "    # Then use a for loop to go through each particle type\n",
    "    for particle_type in particle_dict.values():\n",
    "        # get the list of all row indexes of the current particle_type\n",
    "        candidate_indexes = dataframe[dataframe['id'] == particle_type].index.to_list()\n",
    "        # number of rows to select for this type\n",
    "        n_rows = min_rows_per_type + int((len(candidate_indexes) - min_rows_per_type) ** scaling_power) - 1\n",
    "        print(f'{particle_type}: {len(candidate_indexes)} => {n_rows}')\n",
    "        # select the rows to keep and add them on to the selected rows list\n",
    "        if n_rows > len(candidate_indexes):\n",
    "            selected_indexes.extend(np.random.choice(candidate_indexes, size=n_rows, replace=True))\n",
    "        else:\n",
    "            selected_indexes.extend(np.random.choice(candidate_indexes, size=n_rows, replace=False))\n",
    "    # Confirm that the length of selected indexes = number of particle types (4) x the rows_per_type\n",
    "    # assert len(selected_indexes) == len(particle_dict) * rows_per_type\n",
    "    # Finally use the list of selected indexes to return a balanced version of the dataframe\n",
    "    dataframe_balanced = dataframe.loc[selected_indexes]\n",
    "    return dataframe_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the function immediately like this\n",
    "df_train_balanced = undersample(df_train)\n",
    "# And check the resulting dataset is balanced on type\n",
    "print(df_train_balanced['id'].value_counts())\n",
    "\n",
    "# Then we can change the default number of rows per type\n",
    "df_train_balanced = undersample(df_train, min_rows_per_type=1000)\n",
    "print(df_train_balanced['id'].value_counts())\n",
    "\n",
    "# We know positrons are the smallest group with 1189 so let's use that\n",
    "min_group_size = min(df_train['id'].value_counts())\n",
    "df_train_balanced = undersample(df_train, min_rows_per_type=min_group_size, scaling_power=0)\n",
    "print(df_train_balanced['id'].value_counts())\n",
    "\n",
    "# See what happens when you try and set the rows_per_type larger than 1189\n",
    "# Try and correct this function so it works even if we want more rows than the smallest group has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the training (both balanced and imbalanced) and test datasets to csv files so we can read them in for the modelling\n",
    "df_train.to_csv(f'../data/pid_{ds_size}_train.csv')\n",
    "df_train_balanced.to_csv(f'../data/pid_{ds_size}_train_balanced.csv')\n",
    "df_test.to_csv(f'../data/pid_{ds_size}_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
